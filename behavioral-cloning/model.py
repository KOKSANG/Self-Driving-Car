# -*- coding: utf-8 -*-
"""Behavioural_Cloning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DBDG5-rVHr1OTusGrPf2FNGJQ7mcmduB
"""

import os
import glob
import zipfile
import pathlib
import cv2
import math
import random
import shutil
import skimage as sk
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

from sklearn.model_selection import  train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.utils import shuffle

from keras.models import Sequential
from keras.activations import softmax, relu
from keras.layers import Activation, Dense, Dropout, Flatten, Lambda, Cropping2D, LSTM
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.optimizers import Adam
from keras.regularizers import l2
from keras.callbacks import EarlyStopping, ModelCheckpoint

"""## Ran the new few blocks for my colab configuration, can be ignored."""

from google.colab import drive

drive.mount('/content/gdrive')

!wget https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip

import shutil

shutil.move("/content/data.zip", "/content/gdrive/My Drive/udacity-behavioural-cloning/")

os.chdir('/content/gdrive/My Drive/udacity-behavioural-cloning/')

with zipfile.ZipFile('data.zip') as f:
  f.extractall()

os.chdir('/content/gdrive/My Drive/udacity-behavioural-cloning/data/')

"""## Training code starts here"""

df = pd.read_csv('driving_log.csv')

# Visualizing original distribution

plt.figure(figsize=(15, 3))
hist, bins = np.histogram(df.steering.values, bins=50)
plt.hist(df.steering.values, bins=bins)
plt.title('Steering Distribution Plot')
plt.xlabel('Steering')
plt.ylabel('Count')
plt.show()

# create grayscale image
def grayscale(img):
  return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
# normalize image to zero mean
def normalize(img):
  mean = np.mean(img)
  std = np.std(img)
  return (img-mean)/std
# preprocess with grayscale and normalization
def preprocess(img):
  return normalize(grayscale(img))
# augment image, left right flip for now
def augment(image, randn):
  return np.flip(image, axis=randn%2).astype(np.uint8)
# yeo-johnson bias
def yeo_johnson_bias(steering):
  if steering >= 0:
      return np.log(steering + 1)
  elif steering < 0:
      return -np.log(-steering + 1)

# To separate center, left and right
df_center = pd.concat([df.center, df.steering], axis=1).rename(index=str, columns={'center': 'img'})
df_left = pd.concat([df.left, df.steering], axis=1).rename(index=str, columns={'left': 'img'})
df_right = pd.concat([df.right, df.steering], axis=1).rename(index=str, columns={'right': 'img'})

df_center.head()

# Adjusting the steering value 0 for left and right

for k, v in df_left.iterrows():
  if v.steering == 0:
    df_left.loc[k, 'steering'] = df_left.loc[k, 'steering'] + random.uniform(0.2, 0.5)
  
for k, v in df_right.iterrows():
  if v.steering == 0:
    df_right.loc[k, 'steering'] = df_right.loc[k, 'steering'] + random.uniform(-0.2, -0.5)

new_df = pd.concat([df_center, df_left, df_right], axis=0, ignore_index=True, sort=False)

new_df.tail()

new_df.to_csv('adjusted_log.csv', index=False, encoding='utf-8')

df = pd.read_csv('adjusted_log.csv')

# Visualizing adjusted distribution

plt.figure(figsize=(15, 3))
hist, bins = np.histogram(df.steering.values, bins=50)
plt.hist(df.steering.values, bins=bins)
plt.title('Steering Distribution Plot')
plt.xlabel('Steering')
plt.ylabel('Count')
plt.show()

df.plot(figsize=(15, 3))

df.shape

# Grouping all images and steering together, to do a train test splitting

images = df.img.tolist()
steering = df.steering.tolist()

img_list = []
for img, angle in zip(images, steering):
  row = [img, angle]
  img_list.append(row)

train_samples, validation_samples = train_test_split(img_list, test_size=0.2)

# Data generator

def generator(samples, batch_size=32):
    cwd = os.getcwd()
    num_samples = len(samples)
    while True: # Loop forever so the generator never terminates
        shuffle(samples)
        for offset in range(0, num_samples, batch_size):
            batch_samples = samples[offset:offset+batch_size]

            images = []
            angles = []

            for batch_sample in batch_samples:
                name = os.path.join(cwd, batch_sample[0].strip())
                try:
                  # normalizing image
                  image = normalize(mpimg.imread(name))
                  # reshaping image into its rgb form
                  image = np.reshape(image, (image.shape[0], image.shape[1], 3))
                  steering = float(batch_sample[1])
                  images.append(image)
                  angles.append(steering)
                # if image not found, skip the image
                except FileNotFoundError as msg:
                  print(msg)
                  continue

            # trim image to only see section with road|
            X_train = np.array(images)
            y_train = np.array(angles)
            yield shuffle(X_train, y_train)

# Set our batch size
batch_size = 32

# compile and train the model using the generator function
train_generator = generator(train_samples, batch_size=batch_size)
validation_generator = generator(validation_samples, batch_size=batch_size)



### PART 3: TRAINING ###
# Training Architecture: inspired by NVIDIA architecture #
INPUT_SHAPE = (160, 320, 3)
model = Sequential()
model.add(Cropping2D(cropping=((70,25), (0, 0)), input_shape=INPUT_SHAPE))

model.add(Conv2D(filters=24, kernel_size=5, strides=(2, 2), activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(filters=36, kernel_size=5, strides=(2, 2), activation='relu'))
#model.add(BatchNormalization())
model.add(Conv2D(filters=48, kernel_size=5, strides=(2, 2), activation='relu'))
#model.add(BatchNormalization())
model.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation='relu'))
#model.add(BatchNormalization())
model.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation='relu'))

model.add(Flatten())
model.add(Dense(1164, activation='relu'))
model.add(Dropout(rate=0.5))
model.add(Dense(100, activation='relu'))
model.add(Dropout(rate=0.5))
model.add(Dense(50, activation='relu'))
model.add(Dropout(rate=0.5))
model.add(Dense(10, activation='relu'))
model.add(Dense(1))
adam = Adam(lr = 0.0001)
model.compile(optimizer= adam, loss='mse', metrics=['accuracy'])
model.summary()

history = model.fit_generator(generator=train_generator, steps_per_epoch=math.ceil(len(train_samples)/ batch_size), \
                    epochs=15, verbose=1, validation_data=validation_generator, \
                    validation_steps=math.ceil(len(validation_samples)/ batch_size), use_multiprocessing=False)

print('Done Training')

###Saving Model and Weights###
model_json = model.to_json()
with open("model5.json", "w") as json_file:
  json_file.write(model_json)
model.save('model5.h5')
model.save_weights("model_weights5.h5")
print("Saved model to disk")

### print the keys contained in the history object
print(history.history.keys())

### plot the training and validation loss for each epoch
plt.figure(figsize=(15, 3))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model mean squared error loss')
plt.ylabel('mean squared error loss')
plt.xlabel('epoch')
plt.legend(['training set', 'validation set'], loc='upper right')
plt.show()

